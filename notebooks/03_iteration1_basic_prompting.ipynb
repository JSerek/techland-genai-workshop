{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Iteracja 1: Podstawowe promptowanie (Zero-shot)\n",
    "\n",
    "**Czas:** ~35 minut  \n",
    "**Poziom:** Wej≈õciowy\n",
    "\n",
    "---\n",
    "\n",
    "## Cel iteracji\n",
    "\n",
    "Nauczyƒá siƒô wysy≈Çaƒá zapytania do modelu jƒôzykowego przez API i klasyfikowaƒá recenzje przy u≈ºyciu **zero-shot promptowania** - bez podawania przyk≈Çad√≥w.\n",
    "\n",
    "---\n",
    "\n",
    "## Teoria: Anatomia promptu\n",
    "\n",
    "Prompt to instrukcja, kt√≥rƒÖ wysy≈Çamy do modelu. Sk≈Çada siƒô z dw√≥ch kluczowych czƒô≈õci:\n",
    "\n",
    "| Rola | Opis | Kiedy u≈ºywaƒá |\n",
    "|------|------|--------------|\n",
    "| `system` | Definiuje rolƒô i zasady modelu | Instrukcje og√≥lne, persona, ograniczenia |\n",
    "| `user` | Konkretne zadanie do wykonania | Dane do przetworzenia, pytania |\n",
    "\n",
    "**Dlaczego rozdzielamy `system` od `user`?**\n",
    "\n",
    "- Separacja odpowiedzialno≈õci - `system` to \"kto jest model\", `user` to \"co ma zrobiƒá\"\n",
    "- Pozwala na ≈Çatwe reuznanie tej samej konfiguracji modelu z r√≥≈ºnymi danymi wej≈õciowymi\n",
    "- Modele sƒÖ trenowane z tym podzia≈Çem - daje lepsze wyniki\n",
    "\n",
    "```python\n",
    "# ‚ùå Z≈ÅE: Mieszanie roli i zadania\n",
    "{\"role\": \"user\", \"content\": \"Jeste≈õ ekspertem. Sklasyfikuj tekst.\"}\n",
    "\n",
    "# ‚úÖ DOBRE: Separacja\n",
    "{\"role\": \"system\", \"content\": \"Jeste≈õ ekspertem od analizy tekstu.\"}\n",
    "{\"role\": \"user\", \"content\": \"Sklasyfikuj poni≈ºszy tekst.\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Zero-shot promptowanie\n",
    "\n",
    "**Zero-shot** = model klasyfikuje bez ≈ºadnych przyk≈Çad√≥w. Polegamy wy≈ÇƒÖcznie na wiedzy modelu i jako≈õci naszych instrukcji.\n",
    "\n",
    "**Zalety:** Prosto, szybko, nie wymaga przygotowania przyk≈Çad√≥w  \n",
    "**Wady:** Mniejsza dok≈Çadno≈õƒá, model mo≈ºe interpretowaƒá kategorie po swojemu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup - ≈õrodowisko\n",
    "\n",
    "Ta kom√≥rka wykrywa czy jeste≈õ w **Google Colab** czy lokalnie, a nastƒôpnie:\n",
    "1. Klonuje repozytorium z GitHub (tylko Colab)\n",
    "2. Instaluje wymagane biblioteki\n",
    "3. Konfiguruje ≈õcie≈ºki import√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# Wykryj ≈õrodowisko\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"{'üü° Google Colab' if IN_COLAB else 'üíª Lokalne ≈õrodowisko'}\")\n",
    "\n",
    "# ============================================================\n",
    "# KONFIGURACJA REPO (zmie≈Ñ URL na w≈Ça≈õciwe przed warsztatami)\n",
    "# ============================================================\n",
    "REPO_URL = \"https://github.com/JSerek/techland-genai-workshop.git\"\n",
    "REPO_DIR = Path(\"/content/szkolenie_techland\") if IN_COLAB else Path(\".\").resolve().parent\n",
    "# ============================================================\n",
    "\n",
    "if IN_COLAB:\n",
    "    if not REPO_DIR.exists():\n",
    "        print(f\"üì• Klonujƒô repo...\")\n",
    "        subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
    "        print(\"‚úÖ Repo sklonowane\")\n",
    "    else:\n",
    "        print(\"üîÑ Aktualizujƒô repo (git pull)...\")\n",
    "        subprocess.run([\"git\", \"-C\", str(REPO_DIR), \"pull\"], check=True)\n",
    "\n",
    "    # Dodaj repo do sys.path\n",
    "    sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "    # Zainstaluj zale≈ºno≈õci\n",
    "    print(\"üì¶ Instalujƒô biblioteki...\")\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(REPO_DIR / \"requirements.txt\"), \"-q\"],\n",
    "        check=True\n",
    "    )\n",
    "    print(\"‚úÖ Biblioteki zainstalowane\")\n",
    "else:\n",
    "    # Lokalnie: dodaj root projektu do sys.path\n",
    "    sys.path.insert(0, str(REPO_DIR))\n",
    "    print(f\"üìÇ ≈öcie≈ºka projektu: {REPO_DIR}\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup zako≈Ñczony\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üîë Konfiguracja API\n",
    "# W Google Colab: dodaj sekrety w menu po lewej (üîë ikona)\n",
    "#   Nazwy sekret√≥w: VERTEX_AI_API_KEY, VERTEX_AI_BASE_URL, MODEL_NAME\n",
    "#\n",
    "# Lokalnie: ustaw zmienne ≈õrodowiskowe lub wpisz warto≈õci bezpo≈õrednio\n",
    "# ============================================================\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import userdata\n",
    "    API_KEY   = userdata.get(\"VERTEX_AI_API_KEY\")\n",
    "    BASE_URL  = userdata.get(\"VERTEX_AI_BASE_URL\")\n",
    "    MODEL_NAME = userdata.get(\"MODEL_NAME\") or \"google/gemini-2.5-flash-lite\"\n",
    "else:\n",
    "    import os\n",
    "    API_KEY   = os.environ.get(\"VERTEX_AI_API_KEY\", \"TODO: wklej API key\")\n",
    "    BASE_URL  = os.environ.get(\"VERTEX_AI_BASE_URL\", \"TODO: wklej endpoint URL\")\n",
    "    MODEL_NAME = os.environ.get(\"MODEL_NAME\", \"google/gemini-2.5-flash-lite\")\n",
    "\n",
    "# Walidacja\n",
    "if not API_KEY or API_KEY == \"TODO: wklej API key\":\n",
    "    print(\"‚ùå Brak API_KEY! Dodaj sekret 'VERTEX_AI_API_KEY' w Colab Secrets.\")\n",
    "elif not BASE_URL or BASE_URL == \"TODO: wklej endpoint URL\":\n",
    "    print(\"‚ùå Brak BASE_URL! Dodaj sekret 'VERTEX_AI_BASE_URL' w Colab Secrets.\")\n",
    "else:\n",
    "    print(f\"‚úÖ API skonfigurowane: {MODEL_NAME}\")\n",
    "    print(f\"   Endpoint: {BASE_URL[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja klienta LLM - dane pobrane z Secrets w kom√≥rce powy≈ºej\n",
    "from src.utils.llm_client import create_client, LLMProvider\n",
    "\n",
    "PROVIDER = LLMProvider.VERTEX_AI  # Zmie≈Ñ je≈õli u≈ºywasz innego providera\n",
    "\n",
    "client = create_client(\n",
    "    provider=PROVIDER,\n",
    "    api_key=API_KEY,\n",
    "    base_url=BASE_URL,\n",
    ")\n",
    "print(f\"‚úÖ Klient LLM gotowy: {PROVIDER.value} / {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ ≈Åadowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ≈öcie≈ºki do danych (wzglƒôdem root repo)\n",
    "DATA_DIR = REPO_DIR / \"data\"\n",
    "\n",
    "# Pr√≥bka recenzji\n",
    "df = pd.read_csv(DATA_DIR / \"processed\" / \"workshop_sample.csv\")\n",
    "review_texts = df[\"review_text\"].tolist()\n",
    "print(f\"‚úÖ Za≈Çadowano {len(review_texts)} recenzji\")\n",
    "\n",
    "# Golden dataset\n",
    "golden_path = DATA_DIR / \"evaluation\" / \"golden_dataset.json\"\n",
    "if golden_path.exists():\n",
    "    with open(golden_path) as f:\n",
    "        golden_data = json.load(f)\n",
    "    golden_texts = [item[\"review_text\"] for item in golden_data]\n",
    "    golden_labels = [item[\"labels\"] for item in golden_data]\n",
    "    print(f\"‚úÖ Golden dataset: {len(golden_data)} przyk≈Çad√≥w\")\n",
    "else:\n",
    "    golden_texts, golden_labels = [], []\n",
    "    print(\"‚ö†Ô∏è  Golden dataset nie znaleziony - ewaluacja niedostƒôpna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj golden dataset (je≈õli gotowy)\n",
    "# ============================================================\n",
    "# TODO: Uzupe≈Çnij po przygotowaniu golden dataset\n",
    "# ============================================================\n",
    "GOLDEN_DATASET_PATH = \"../data/evaluation/golden_dataset.json\"  # TODO\n",
    "\n",
    "golden_path = Path(GOLDEN_DATASET_PATH)\n",
    "if golden_path.exists():\n",
    "    with open(golden_path) as f:\n",
    "        golden_data = json.load(f)\n",
    "    golden_texts = [item[\"review_text\"] for item in golden_data]\n",
    "    golden_labels = [item[\"labels\"] for item in golden_data]\n",
    "    print(f\"‚úÖ Golden dataset: {len(golden_data)} przyk≈Çad√≥w\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Golden dataset nie znaleziony - ewaluacja niedostƒôpna\")\n",
    "    golden_texts, golden_labels = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Zadanie: Napisz prompt klasyfikujƒÖcy\n",
    "\n",
    "### Twoje zadanie:\n",
    "\n",
    "Napisz prompt, kt√≥ry klasyfikuje recenzjƒô gry do jednej lub wiƒôcej kategorii tematycznych.\n",
    "\n",
    "> üöß **TODO:** Uzupe≈Çnij kategorie po analizie danych\n",
    "\n",
    "**Wskaz√≥wki do dobrego promptu:**\n",
    "- Podaj dok≈Çadny format odpowiedzi (co model ma zwr√≥ciƒá)\n",
    "- Zdefiniuj kategorie precyzyjnie - model musi wiedzieƒá co oznaczajƒÖ\n",
    "- Ogranicz model tylko do zdefiniowanych kategorii\n",
    "- Testuj na kilku przyk≈Çadach zanim pu≈õcisz na ca≈Çym zbiorze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# KONFIGURACJA kategorii\n",
    "# TODO: Uzupe≈Çnij po analizie danych\n",
    "# ============================================================\n",
    "CATEGORIES = [\n",
    "    # Przyk≈Çadowy format - zmie≈Ñ na w≈Ça≈õciwe kategorie:\n",
    "    # \"performance\",    # Problemy z wydajno≈õciƒÖ, FPS, loading\n",
    "    # \"bug\",            # B≈Çƒôdy, crashe, glitche\n",
    "    # \"story\",          # Fabu≈Ça, narracja, postacie\n",
    "    # \"gameplay\",       # Mechaniki rozgrywki, sterowanie\n",
    "    # \"graphics\",       # Grafika, oprawa wizualna\n",
    "    # \"audio\",          # Muzyka, d≈∫wiƒôki, dubbing\n",
    "    # \"price\",          # Cena, warto≈õƒá za pieniƒÖdze\n",
    "    # \"other\",          # Inne\n",
    "]\n",
    "\n",
    "categories_str = \", \".join(f'\"{c}\"' for c in CATEGORIES)\n",
    "print(f\"Dostƒôpne kategorie: {categories_str or '‚ö†Ô∏è  Brak - uzupe≈Çnij CATEGORIES'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# üñäÔ∏è  ƒÜWICZENIE: Napisz sw√≥j prompt\n",
    "# Zmodyfikuj poni≈ºszy szablon - zmie≈Ñ tre≈õƒá, dodaj/usu≈Ñ instrukcje\n",
    "# ============================================================\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"Jeste≈õ ekspertem od analizy recenzji gier komputerowych.\n",
    "Twoim zadaniem jest klasyfikacja tematyczna recenzji.\n",
    "\n",
    "Dostƒôpne kategorie: {categories}\n",
    "\n",
    "Zasady:\n",
    "- Mo≈ºesz przypisaƒá jednƒÖ lub wiƒôcej kategorii\n",
    "- U≈ºywaj tylko kategorii z powy≈ºszej listy\n",
    "- Odpowiedz WY≈ÅƒÑCZNIE listƒÖ kategorii oddzielonych przecinkami, bez ≈ºadnego komentarza\n",
    "\n",
    "Przyk≈Çad formatu odpowiedzi: bug, performance\"\"\".format(categories=categories_str)\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Sklasyfikuj poni≈ºszƒÖ recenzjƒô gry:\n",
    "\n",
    "Recenzja: {review_text}\n",
    "\n",
    "Kategorie:\"\"\"\n",
    "\n",
    "print(\"SYSTEM PROMPT:\")\n",
    "print(SYSTEM_PROMPT)\n",
    "print(\"\\nUSER PROMPT TEMPLATE:\")\n",
    "print(USER_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Test na jednej recenzji\n",
    "\n",
    "Zawsze najpierw przetestuj na jednym przyk≈Çadzie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_review(review_text: str) -> list[str]:\n",
    "    \"\"\"Klasyfikuje recenzjƒô do kategorii tematycznych.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": USER_PROMPT_TEMPLATE.format(review_text=review_text)},\n",
    "        ],\n",
    "        temperature=0.0,  # 0 = deterministyczny output (lepsza reprodukowalno≈õƒá)\n",
    "    )\n",
    "\n",
    "    raw_output = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Parsuj odpowied≈∫: \"bug, performance\" ‚Üí [\"bug\", \"performance\"]\n",
    "    labels = [label.strip().lower() for label in raw_output.split(\",\") if label.strip()]\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Test na pierwszej recenzji\n",
    "test_review = review_texts[0]\n",
    "print(f\"Recenzja: '{test_review[:300]}...'\")\n",
    "print()\n",
    "\n",
    "result = classify_review(test_review)\n",
    "print(f\"Klasyfikacja: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Klasyfikacja ca≈Çego zbioru\n",
    "\n",
    "Uruchom klasyfikacjƒô na ca≈Çym golden dataset (lub pr√≥bce)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# U≈ºyj golden dataset (lub pr√≥bki je≈õli golden nie gotowy)\n",
    "texts_to_classify = golden_texts if golden_texts else review_texts[:10]\n",
    "\n",
    "print(f\"Klasyfikujƒô {len(texts_to_classify)} recenzji...\")\n",
    "\n",
    "predictions = []\n",
    "errors = []\n",
    "\n",
    "for i, text in enumerate(tqdm(texts_to_classify)):\n",
    "    try:\n",
    "        labels = classify_review(text)\n",
    "        predictions.append(labels)\n",
    "    except Exception as e:\n",
    "        print(f\"B≈ÇƒÖd dla recenzji {i}: {e}\")\n",
    "        predictions.append([])  # pusta lista = brak klasyfikacji\n",
    "        errors.append(i)\n",
    "\n",
    "print(f\"\\n‚úÖ Klasyfikacja zako≈Ñczona. B≈Çƒôdy: {len(errors)}/{len(texts_to_classify)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Ewaluacja wynik√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if golden_labels:\n",
    "    trial1 = evaluate_trial(\n",
    "        trial_name=f\"{MODEL_NAME} + zero-shot v1\",\n",
    "        model=MODEL_NAME,\n",
    "        prompt_variant=\"zero-shot v1\",\n",
    "        predictions=predictions,\n",
    "        expected=golden_labels,\n",
    "        review_texts=golden_texts,\n",
    "        strategy=MatchStrategy.CONTAINS_ALL,\n",
    "    )\n",
    "\n",
    "    # Wy≈õwietl wyniki\n",
    "    trial1.display()\n",
    "\n",
    "    # Zapisz wyniki do por√≥wnania w kolejnych iteracjach\n",
    "    print(f\"\\nüíæ Accuracy Iteracja 1: {trial1.summary.accuracy:.1%}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Golden dataset nie jest gotowy - pokazujƒô tylko predykcje:\")\n",
    "    for text, pred in zip(texts_to_classify[:5], predictions[:5]):\n",
    "        print(f\"  Recenzja: '{text[:100]}...'\")\n",
    "        print(f\"  Kategorie: {pred}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° ƒÜwiczenie dla uczestnik√≥w\n",
    "\n",
    "### Zadanie:\n",
    "Zmodyfikuj `SYSTEM_PROMPT` i `USER_PROMPT_TEMPLATE` tak, ≈ºeby poprawiƒá accuracy klasyfikacji.\n",
    "\n",
    "**Pomys≈Çy do eksperymentowania:**\n",
    "1. Dodaj definicje kategorii (co dok≈Çadnie nale≈ºy do ka≈ºdej)\n",
    "2. Zmie≈Ñ format odpowiedzi (np. JSON zamiast CSV)\n",
    "3. Dodaj instrukcjƒô \"If unsure, use 'other'\"\n",
    "4. Zmie≈Ñ jƒôzyk promptu (angielski vs polski)\n",
    "5. Dodaj/usu≈Ñ przyk≈Çady formatu odpowiedzi\n",
    "\n",
    "**Pytania do refleksji:**\n",
    "- Kt√≥re recenzje model klasyfikuje najgorzej?\n",
    "- Czy model nadaje zbyt wiele kategorii, czy za ma≈Ço?\n",
    "- Jak wp≈Çywa zmiana temperatury (0.0 ‚Üí 0.7) na wyniki?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miejsce na Tw√≥j eksperyment - skopiuj i zmodyfikuj prompty powy≈ºej\n",
    "MY_SYSTEM_PROMPT = \"\"\"\n",
    "# TODO: Wpisz sw√≥j system prompt tutaj\n",
    "\"\"\"\n",
    "\n",
    "MY_USER_PROMPT_TEMPLATE = \"\"\"\n",
    "# TODO: Wpisz sw√≥j user prompt tutaj\n",
    "\"\"\"\n",
    "\n",
    "# Gdy gotowy - uruchom klasyfikacjƒô i ewaluacjƒô:\n",
    "# ...\n",
    "print(\"Zmodyfikuj prompty powy≈ºej i uruchom klasyfikacjƒô!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Podsumowanie Iteracji 1\n",
    "\n",
    "**Czego siƒô nauczy≈Çe≈õ:**\n",
    "- Struktura promptu: `system` + `user` i dlaczego to wa≈ºne\n",
    "- Zero-shot classificationvia API\n",
    "- Parsowanie surowego stringa z odpowiedziƒÖ modelu\n",
    "- Ewaluacja accuracy z golden dataset\n",
    "\n",
    "**Problem do rozwiƒÖzania w Iteracji 2:**  \n",
    "Surowy string z modelu jest trudny do parsowania i mo≈ºe mieƒá r√≥≈ºne formaty.  \n",
    "‚Üí RozwiƒÖzanie: **Structured Output z Pydantic**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}