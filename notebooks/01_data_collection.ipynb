{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam Reviews Data Collection\n",
    "\n",
    "**Dying Light 2: The Beast - Review Scraper**\n",
    "\n",
    "This notebook demonstrates how to scrape Steam game reviews for training data collection.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the required libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-reload modules (useful during development)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path (for Google Colab compatibility)\n",
    "if 'google.colab' in sys.modules:\n",
    "    # In Colab, mount drive and navigate to project\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Update this path to your project location in Google Drive\n",
    "    project_path = Path('/content/drive/MyDrive/szkolenie_techland')\n",
    "    sys.path.insert(0, str(project_path))\n",
    "else:\n",
    "    # Local execution\n",
    "    project_path = Path('.').parent\n",
    "    sys.path.insert(0, str(project_path))\n",
    "\n",
    "print(f\"Project path: {project_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom modules\n",
    "from src.scraper.steam_api import SteamReviewScraper, quick_scrape\n",
    "from src.scraper.utils import (\n",
    "    save_to_formats,\n",
    "    get_review_statistics,\n",
    ")\n",
    "from src.utils.config import (\n",
    "    DYING_LIGHT_BEAST_APP_ID,\n",
    "    RAW_DATA_DIR,\n",
    ")\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✅ All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Explore Steam API\n",
    "\n",
    "Let's first test the API connection and see what a single page of reviews looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scraper for Dying Light 2: The Beast\n",
    "scraper = SteamReviewScraper(app_id=DYING_LIGHT_BEAST_APP_ID)\n",
    "\n",
    "print(f\"Steam API Endpoint: {scraper.endpoint}\")\n",
    "print(f\"App ID: {scraper.app_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a single page of reviews to explore the structure\n",
    "print(\"Fetching first page of negative reviews...\\n\")\n",
    "\n",
    "batch = scraper.fetch_review_page(\n",
    "    cursor=\"*\",\n",
    "    review_type=\"negative\",\n",
    "    language=\"english\",\n",
    "    num_per_page=10,  # Small sample\n",
    ")\n",
    "\n",
    "print(f\"✅ Successfully fetched {len(batch)} reviews\")\n",
    "print(f\"\\nQuery Summary:\")\n",
    "print(f\"  Total reviews in database: {batch.query_summary.total_reviews:,}\")\n",
    "print(f\"  Positive: {batch.query_summary.total_positive:,}\")\n",
    "print(f\"  Negative: {batch.query_summary.total_negative:,}\")\n",
    "print(f\"  Review score: {batch.query_summary.review_score_desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the first review in detail\n",
    "if batch.reviews:\n",
    "    review = batch.reviews[0]\n",
    "    \n",
    "    print(\"Sample Review:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Review ID: {review.recommendationid}\")\n",
    "    print(f\"Sentiment: {review.sentiment} (voted_up={review.voted_up})\")\n",
    "    print(f\"Language: {review.language}\")\n",
    "    print(f\"Playtime: {review.playtime_hours} hours\")\n",
    "    print(f\"Votes Up: {review.votes_up}\")\n",
    "    print(f\"Created: {review.created_date}\")\n",
    "    print(f\"\\nReview Text:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(review.review)\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Scrape Reviews\n",
    "\n",
    "Now let's scrape a larger dataset of reviews. We'll start with a small sample for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for scraping\n",
    "MAX_REVIEWS = 1000  # Start with 1k for testing, change to 100000 for full scrape\n",
    "REVIEW_TYPE = \"negative\"  # Focus on negative reviews\n",
    "LANGUAGE = \"english\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Target reviews: {MAX_REVIEWS:,}\")\n",
    "print(f\"  Review type: {REVIEW_TYPE}\")\n",
    "print(f\"  Language: {LANGUAGE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape reviews\n",
    "print(\"Starting review scraping...\\n\")\n",
    "\n",
    "reviews = scraper.scrape_reviews(\n",
    "    max_reviews=MAX_REVIEWS,\n",
    "    review_type=REVIEW_TYPE,\n",
    "    language=LANGUAGE,\n",
    "    save_checkpoints=True,\n",
    "    checkpoint_interval=5000,\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Scraped {len(reviews):,} reviews!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Quick Statistics\n",
    "\n",
    "Let's analyze the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics\n",
    "stats = get_review_statistics(reviews)\n",
    "scraper_stats = scraper.get_stats_summary()\n",
    "\n",
    "print(\"Review Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nScraper Performance:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in scraper_stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame([r.to_dict_simplified() for r in reviews])\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sentiment counts\n",
    "df['sentiment'].value_counts().plot(\n",
    "    kind='bar',\n",
    "    ax=axes[0],\n",
    "    color=['#e74c3c', '#2ecc71']\n",
    ")\n",
    "axes[0].set_title('Review Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Votes distribution\n",
    "df['votes_up'].hist(bins=30, ax=axes[1], color='#3498db', edgecolor='black')\n",
    "axes[1].set_title('Distribution of Helpful Votes', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Votes Up')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playtime analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Playtime distribution\n",
    "playtime_filtered = df[df['playtime_hours'] < 200]  # Filter outliers for better viz\n",
    "playtime_filtered['playtime_hours'].hist(\n",
    "    bins=40,\n",
    "    ax=axes[0],\n",
    "    color='#9b59b6',\n",
    "    edgecolor='black'\n",
    ")\n",
    "axes[0].set_title('Playtime Distribution (< 200h)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Playtime (hours)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Playtime by sentiment\n",
    "df.boxplot(\n",
    "    column='playtime_hours',\n",
    "    by='sentiment',\n",
    "    ax=axes[1],\n",
    "    patch_artist=True\n",
    ")\n",
    "axes[1].set_title('Playtime by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_ylabel('Playtime (hours)')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review length analysis\n",
    "df['review_length'] = df['review_text'].str.len()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Filter very long reviews for better visualization\n",
    "df_filtered = df[df['review_length'] < 2000]\n",
    "\n",
    "df_filtered.boxplot(\n",
    "    column='review_length',\n",
    "    by='sentiment',\n",
    "    ax=ax,\n",
    "    patch_artist=True\n",
    ")\n",
    "ax.set_title('Review Length by Sentiment (< 2000 chars)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Review Length (characters)')\n",
    "plt.suptitle('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average review length:\")\n",
    "print(df.groupby('sentiment')['review_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top voted reviews (most helpful)\n",
    "print(\"Top 5 Most Helpful Negative Reviews:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "top_reviews = df.nlargest(5, 'votes_up')\n",
    "\n",
    "for idx, row in top_reviews.iterrows():\n",
    "    print(f\"\\n[{row['votes_up']} votes | {row['playtime_hours']}h played]\")\n",
    "    print(f\"{row['review_text'][:300]}...\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of reviews\n",
    "df['created_date'] = pd.to_datetime(df['created_date'])\n",
    "df['date'] = df['created_date'].dt.date\n",
    "\n",
    "reviews_by_date = df.groupby('date').size()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "reviews_by_date.plot(kind='line', ax=ax, color='#e67e22', linewidth=2)\n",
    "ax.set_title('Reviews Over Time', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Number of Reviews')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Export Data\n",
    "\n",
    "Save the scraped reviews to multiple formats for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output path\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_name = f\"dying_light_beast_{REVIEW_TYPE}_{timestamp}\"\n",
    "output_base = RAW_DATA_DIR / output_name\n",
    "\n",
    "print(f\"Saving reviews to: {output_base}\")\n",
    "\n",
    "# Save to multiple formats\n",
    "saved_files = save_to_formats(\n",
    "    reviews,\n",
    "    output_base,\n",
    "    formats=['json', 'csv', 'parquet']\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Data exported successfully!\")\n",
    "print(\"\\nSaved files:\")\n",
    "for fmt, path in saved_files.items():\n",
    "    print(f\"  {fmt.upper()}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Sample Reviews for Manual Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample of reviews for manual inspection\n",
    "sample = df.sample(10, random_state=42)\n",
    "\n",
    "print(\"Random Sample of 10 Reviews:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"\\n{row['sentiment'].upper()} | {row['playtime_hours']}h | {row['votes_up']} votes\")\n",
    "    print(f\"{row['review_text'][:400]}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. ✅ Connected to Steam API and fetched review data\n",
    "2. ✅ Scraped reviews with rate limiting and error handling\n",
    "3. ✅ Analyzed review statistics and patterns\n",
    "4. ✅ Visualized data distributions\n",
    "5. ✅ Exported data in multiple formats (JSON, CSV, Parquet)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Data Cleaning** (`02_data_cleaning.ipynb`): Remove duplicates, filter low-quality reviews, balance dataset\n",
    "2. **Classification Setup**: Define categories and prepare evaluation dataset\n",
    "3. **LLM Classification**: Start with iteration 1 - basic prompting\n",
    "\n",
    "---\n",
    "\n",
    "### Notes:\n",
    "\n",
    "- To scrape more reviews, increase `MAX_REVIEWS` in the configuration cell\n",
    "- Checkpoints are saved automatically every 5,000 reviews\n",
    "- You can resume scraping by using the `resume_from_checkpoint=True` parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
